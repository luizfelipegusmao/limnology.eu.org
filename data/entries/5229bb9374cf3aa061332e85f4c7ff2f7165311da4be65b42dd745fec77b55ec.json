{"title":"Generating Real-Time Audio Sentiment Analysis With AI","link":"https://smashingmagazine.com/2023/09/generating-real-time-audio-sentiment-analysis-ai/","date":1693832400000,"content":"<p>In the <a href=\"https://www.smashingmagazine.com/2023/06/ai-detect-sentiment-audio-files/\">previous article</a>, we developed a sentiment analysis tool that could detect and score emotions hidden within audio files. Weâ€™re taking it to the next level in this article by integrating real-time analysis and multilingual support. Imagine analyzing the sentiment of your audio content in real-time as the audio file is transcribed. In other words, the tool we are building offers immediate insights as an audio file plays.</p>\n<p>So, how does it all come together? Meet <a href=\"https://openai.com/research/whisper\">Whisper</a> and <a href=\"https://www.gradio.app/\">Gradio</a> â€” the two resources that sit under the hood. Whisper is an advanced automatic speech recognition and language detection library. It swiftly converts audio files to text and identifies the language. Gradio is a UI framework that happens to be designed for interfaces that utilize machine learning, which is ultimately what we are doing in this article. With Gradio, you can create user-friendly interfaces without complex installations, configurations, or any machine learning experience â€” the perfect tool for a tutorial like this.</p>\n<p>By the end of this article, we will have created a fully-functional app that:</p>\n<ul>\n<li>Records audio from the userâ€™s microphone,</li>\n<li>Transcribes the audio to plain text,</li>\n<li>Detects the language,</li>\n<li>Analyzes the emotional qualities of the text, and</li>\n<li>Assigns a score to the result.</li>\n</ul>\n<p><strong>Note</strong>: <em>You can peek at the final product in the <a href=\"https://huggingface.co/spaces/Pontonkid/Real-Time-Multilingual-sentiment-analysis\">live demo</a>.</em></p>\nAutomatic Speech Recognition And Whisper\n<p>Letâ€™s delve into the fascinating world of <strong>automatic speech recognition</strong> and its ability to analyze audio. In the process, weâ€™ll also introduce Whisper, an automated speech recognition tool developed by the OpenAI team behind ChatGPT and other emerging artificial intelligence technologies. Whisper has redefined the field of speech recognition with its innovative capabilities, and weâ€™ll closely examine its available features.</p>\nAutomatic Speech Recognition (ASR)\n<p>ASR technology is a key component for converting speech to text, making it a valuable tool in todayâ€™s digital world. Its applications are vast and diverse, spanning various industries. ASR can efficiently and accurately transcribe audio files into plain text. It also powers voice assistants, enabling seamless interaction between humans and machines through spoken language. Itâ€™s used in myriad ways, such as in call centers that automatically route calls and provide callers with self-service options.</p>\n<p>By automating audio conversion to text, ASR significantly saves time and boosts productivity across multiple domains. Moreover, it opens up new avenues for data analysis and decision-making.</p>\n<p>That said, ASR does have its fair share of challenges. For example, its accuracy is diminished when dealing with different accents, background noises, and speech variations â€” all of which require innovative solutions to ensure accurate and reliable transcription. The development of ASR systems capable of handling diverse audio sources, adapting to multiple languages, and maintaining exceptional accuracy is crucial for overcoming these obstacles.</p>\nWhisper: A Speech Recognition Model\n<p><a href=\"https://openai.com/research/whisper\">Whisper</a> is a speech recognition model also developed by OpenAI. This powerful model excels at speech recognition and offers language identification and translation across multiple languages. Itâ€™s an open-source model available in five different sizes, four of which have an English-only variant that performs exceptionally well for single-language tasks.</p>\n<p>What sets Whisper apart is its robust ability to overcome ASR challenges. Whisper achieves near state-of-the-art performance and even supports <a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning\">zero-shot translation</a> from various languages to English. Whisper has been trained on a large corpus of data that characterizes ASRâ€™s challenges. The training data consists of approximately 680,000 hours of multilingual and multitask supervised data collected from the web.</p>\n<p>The model is available in multiple sizes. The following table outlines these model characteristics:</p>\n<table>\n    <thead>\n        <tr>\n            <th>Size</th>\n            <th>Parameters</th>\n      <th>English-only model</th>\n      <th>Multilingual model</th>\n      <th>Required VRAM</th>\n      <th>Relative speed</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Tiny</td>\n            <td>39 M</td>\n      <td><code>tiny.en</code></td>\n      <td>tiny</td>\n      <td>~1 GB</td>\n      <td>~32x</td>\n        </tr>\n        <tr>\n            <td>Base</td>\n            <td>74 M</td>\n      <td><code>base.en</code></td>\n      <td>base</td>\n      <td>~1 GB</td>\n      <td>~16x</td>\n        </tr>\n        <tr>\n            <td>Small</td>\n            <td>244 M</td>\n      <td><code>small.en</code></td>\n      <td>small</td>\n      <td>~2 GB</td>\n      <td>~6x</td>\n        </tr>\n    <tr>\n            <td>Medium</td>\n            <td>769 M</td>\n      <td><code>medium.en</code></td>\n      <td>medium</td>\n      <td>~5 GB</td>\n      <td>~2x</td>\n        </tr>\n    <tr>\n            <td>Large</td>\n            <td>1550 M</td>\n      <td>N/A</td>\n      <td>large</td>\n      <td>~10 GB</td>\n      <td>1x</td>\n        </tr>\n    </tbody>\n</table>\n\n<p>For developers working with English-only applications, itâ€™s essential to consider the performance differences among the <code>.en</code> models â€” specifically, <code>tiny.en</code> and <code>base.en</code>, both of which offer better performance than the other models.</p>\n<p>Whisper utilizes a <a href=\"https://en.wikipedia.org/wiki/Seq2seq\">Seq2seq</a> (i.e., transformer encoder-decoder) architecture commonly employed in language-based models. This architectureâ€™s input consists of audio frames, typically 30-second segment pairs. The output is a sequence of the corresponding text. Its primary strength lies in transcribing audio into text, making it ideal for â€œaudio-to-textâ€ use cases.</p>\n<p><img src=\"https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/1-ai-real-time-audio-analysis-whisper-architecure.png\" /></p>\nReal-Time Sentiment Analysis\n<p>Next, letâ€™s move into the different components of our real-time sentiment analysis app. Weâ€™ll explore a powerful pre-trained language model and an intuitive user interface framework.</p>\n<h3>Hugging Face Pre-Trained Model</h3>\n<p>I relied on the <a href=\"https://huggingface.co/docs/transformers/model_doc/distilbert\">DistilBERT</a> model in my previous article, but weâ€™re trying something new now. To analyze sentiments precisely, weâ€™ll use a pre-trained model called <a href=\"https://huggingface.co/SamLowe/roberta-base-go_emotions\">roberta-base-go_emotions</a>, readily available on the <a href=\"https://huggingface.co/docs/hub/models-the-hub\">Hugging Face Model Hub</a>.</p>\n<h3>Gradio UI Framework</h3>\n<p>To make our application more user-friendly and interactive, Iâ€™ve chosen <a href=\"https://www.gradio.app/\">Gradio</a> as the framework for building the interface. Last time, we used <a href=\"https://github.com/streamlit/streamlit\">Streamlit</a>, so itâ€™s a little bit of a different process this time around. You can use any UI framework for this exercise.</p>\n<p>Iâ€™m using Gradio specifically for its machine learning integrations to keep this tutorial focused more on real-time sentiment analysis than fussing with UI configurations. Gradio is explicitly designed for creating demos just like this, providing everything we need â€” including the language models, APIs, UI components, styles, deployment capabilities, and hosting â€” so that experiments can be created and shared quickly.</p>\nInitial Setup\n<p>Itâ€™s time to dive into the code that powers the sentiment analysis. I will break everything down and walk you through the implementation to help you understand how everything works together.</p>\n<p>Before we start, we must ensure we have the required libraries installed and they can be installed with npm. If you are using <a href=\"https://colab.research.google.com\">Google Colab</a>, you can install the libraries using the following commands:</p>\n<pre><code>!pip install gradio\n!pip install transformers\n!pip install git+https://github.com/openai/whisper.git\n</code></pre>\n\n<p>Once the libraries are installed, we can import the necessary modules:</p>\n<pre><code>import gradio as gr\nimport whisper\nfrom transformers import pipeline\n</code></pre>\n\n<p>This imports Gradio, Whisper, and <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\"><code>pipeline</code></a> from <a href=\"https://huggingface.co/docs/transformers/index\">Transformers</a>, which performs sentiment analysis using pre-trained models.</p>\n<p>Like we did last time, the project folder can be kept relatively small and straightforward. All of the code we are writing can live in an <code>app.py</code> file. Gradio is based on Python, but the UI framework you ultimately use may have different requirements. Again, Iâ€™m using Gradio because it is deeply integrated with machine learning models and APIs, which is ideal for a tutorial like this.</p>\n<p>Gradio projects usually include a <code>requirements.txt</code> file for documenting the app, much like a <code>README</code> file. I would include it, even if it contains no content.</p>\n<p>To set up our application, we load Whisper and initialize the sentiment analysis component in the <code>app.py</code> file:</p>\n<pre><code>model = whisper.load_model(\"base\")\n\nsentiment_analysis = pipeline(\n  \"sentiment-analysis\",\n  framework=\"pt\",\n  model=\"SamLowe/roberta-base-go_emotions\"\n)\n</code></pre>\n\n<p>So far, weâ€™ve set up our application by loading the Whisper model for speech recognition and initializing the sentiment analysis component using a pre-trained model from Hugging Face Transformers.</p>\nDefining Functions For Whisper And Sentiment Analysis\n<p>Next, we must define four functions related to the Whisper and pre-trained sentiment analysis models.</p>\n<h3>Function 1: <code>analyze_sentiment(text)</code></h3>\n<p>This function takes a text input and performs sentiment analysis using the pre-trained sentiment analysis model. It returns a dictionary containing the sentiments and their corresponding scores.</p>\n<pre><code>def analyze_sentiment(text):\n  results = sentiment_analysis(text)\n  sentiment_results = {\n    result[â€™labelâ€™]: result[â€™scoreâ€™] for result in results\n  }\nreturn sentiment_results\n</code></pre>\n\n<h3>Function 2: <code>get_sentiment_emoji(sentiment)</code></h3>\n<p>This function takes a sentiment as input and returns a corresponding emoji used to help indicate the sentiment score. For example, a score that results in an â€œoptimisticâ€ sentiment returns a â€œğŸ˜Šâ€ emoji. So, sentiments are mapped to emojis and return the emoji associated with the sentiment. If no emoji is found, it returns an empty string.</p>\n<pre><code>def get_sentiment_emoji(sentiment):\n  # Define the mapping of sentiments to emojis\n  emoji_mapping = {\n    \"disappointment\": \"ğŸ˜\",\n    \"sadness\": \"ğŸ˜¢\",\n    \"annoyance\": \"ğŸ˜ \",\n    \"neutral\": \"ğŸ˜\",\n    \"disapproval\": \"ğŸ‘\",\n    \"realization\": \"ğŸ˜®\",\n    \"nervousness\": \"ğŸ˜¬\",\n    \"approval\": \"ğŸ‘\",\n    \"joy\": \"ğŸ˜„\",\n    \"anger\": \"ğŸ˜¡\",\n    \"embarrassment\": \"ğŸ˜³\",\n    \"caring\": \"ğŸ¤—\",\n    \"remorse\": \"ğŸ˜”\",\n    \"disgust\": \"ğŸ¤¢\",\n    \"grief\": \"ğŸ˜¥\",\n    \"confusion\": \"ğŸ˜•\",\n    \"relief\": \"ğŸ˜Œ\",\n    \"desire\": \"ğŸ˜\",\n    \"admiration\": \"ğŸ˜Œ\",\n    \"optimism\": \"ğŸ˜Š\",\n    \"fear\": \"ğŸ˜¨\",\n    \"love\": \"â¤ï¸\",\n    \"excitement\": \"ğŸ‰\",\n    \"curiosity\": \"ğŸ¤”\",\n    \"amusement\": \"ğŸ˜„\",\n    \"surprise\": \"ğŸ˜²\",\n    \"gratitude\": \"ğŸ™\",\n    \"pride\": \"ğŸ¦\"\n  }\nreturn emoji_mapping.get(sentiment, \"\")\n</code></pre>\n\n<h3>Function 3: <code>display_sentiment_results(sentiment_results, option)</code></h3>\n<p>This function displays the sentiment results based on a selected option, allowing users to choose how the sentiment score is formatted. Users have two options: show the score with an emoji or the score with an emoji <em>and</em> the calculated score. The function inputs the sentiment results (<code>sentiment</code> and <code>score</code>) and the selected display <code>option</code>, then formats the sentiment and score based on the chosen option and returns the text for the sentiment findings (<code>sentiment_text</code>).</p>\n<pre><code>def display_sentiment_results(sentiment_results, option):\nsentiment_text = \"\"\nfor sentiment, score in sentiment_results.items():\n  emoji = get_sentiment_emoji(sentiment)\n  if option == \"Sentiment Only\":\n    sentiment_text += f\"{sentiment} {emoji}\\n\"\n  elif option == \"Sentiment + Score\":\n    sentiment_text += f\"{sentiment} {emoji}: {score}\\n\"\nreturn sentiment_text\n</code></pre>\n\n<h3>Function 4: <code>inference(audio, sentiment_option)</code></h3>\n<p>This function performs Hugging Faceâ€™s <a href=\"https://huggingface.co/docs/api-inference/index\">inference process</a>, including language identification, speech recognition, and sentiment analysis. It inputs the audio file and sentiment display option from the third function. It returns the language, transcription, and sentiment analysis results that we can use to display all of these in the front-end UI we will make with Gradio in the next section of this article.</p>\n<div>\n<pre><code>def inference(audio, sentiment_option):\n  audio = whisper.load_audio(audio)\n  audio = whisper.pad_or_trim(audio)\n\n  mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n  _, probs = model.detect_language(mel)\n  lang = max(probs, key=probs.get)\n\n  options = whisper.DecodingOptions(fp16=False)\n  result = whisper.decode(model, mel, options)\n\n  sentiment_results = analyze_sentiment(result.text)\n  sentiment_output = display_sentiment_results(sentiment_results, sentiment_option)\n\nreturn lang.upper(), result.text, sentiment_output\n</code></pre>\n</div>\n\n\n\nCreating The User Interface\n<p>Now that we have the foundation for our project â€” Whisper, Gradio, and functions for returning a sentiment analysis â€” in place, all thatâ€™s left is to build the layout that takes the inputs and displays the returned results for the user on the front end.</p>\n<p><img src=\"https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/ai-real-time-audio-analysis-app-ui.png\" /></p>\n<p>The following steps I will outline are specific to Gradioâ€™s UI framework, so your mileage will undoubtedly vary depending on the framework you decide to use for your project.</p>\n<h3>Defining The Header Content</h3>\n<p>Weâ€™ll start with the header containing a title, an image, and a block of text describing how sentiment scoring is evaluated.</p>\n<p>Letâ€™s define variables for those three pieces:</p>\n<div>\n<pre><code>title = \"\"\"ğŸ¤ Multilingual ASR ğŸ’¬\"\"\"\nimage_path = \"/content/thumbnail.jpg\"\n\ndescription = \"\"\"\n  ğŸ’» This demo showcases a general-purpose speech recognition model called Whisper. It is trained on a large dataset of diverse audio and supports multilingual speech recognition and language identification tasks.<br /><br />\n  ğŸ“ For more details, check out the [GitHub repository](<a href=\"https://github.com/openai/whisper)\">https://github.com/openai/whisper)</a>.<br /><br />\n  âš™ï¸ Components of the tool:<br /><br />\n  Â Â Â Â  - Real-time multilingual speech recognition<br />\n  Â Â Â Â  - Language identification<br />\n  Â Â Â Â  - Sentiment analysis of the transcriptions<br /><br />\n  ğŸ¯ The sentiment analysis results are provided as a dictionary with different emotions and their corresponding scores.<br /><br />\n  ğŸ˜ƒ The sentiment analysis results are displayed with emojis representing the corresponding sentiment.<br /><br />\n  âœ… The higher the score for a specific emotion, the stronger the presence of that emotion in the transcribed text.<br /><br />\n  â“ Use the microphone for real-time speech recognition.<br /><br />\n  âš¡ï¸ The model will transcribe the audio and perform sentiment analysis on the transcribed text.<br />\n\"\"\"\n</code></pre>\n</div>\n\n<h3>Applying Custom CSS</h3>\n<p>Styling the layout and UI components is outside the scope of this article, but I think itâ€™s important to demonstrate how to apply custom CSS in a Gradio project. It can be done with a <code>custom_css</code> variable that contains the styles:</p>\n<pre><code>custom_css = \"\"\"\n  #banner-image {\n    display: block;\n    margin-left: auto;\n    margin-right: auto;\n  }\n  #chat-message {\n    font-size: 14px;\n    min-height: 300px;\n  }\n\"\"\"\n</code></pre>\n\n<h3>Creating Gradio Blocks</h3>\n<p>Gradioâ€™s UI framework is based on the concept of <a href=\"https://www.gradio.app/docs/blocks\"><strong>blocks</strong></a>. A block is used to define <a href=\"https://www.gradio.app/docs/block-layouts\">layouts</a>, <a href=\"https://www.gradio.app/docs/components\">components</a>, and events combined to create a complete interface with which users can interact. For example, we can create a block specifically for the custom CSS from the previous step:</p>\n<pre><code>block = gr.Blocks(css=custom_css)\n</code></pre>\n\n<p>Letâ€™s apply our header elements from earlier into the block:</p>\n<pre><code>block = gr.Blocks(css=custom_css)\n\nwith block:\n  gr.HTML(title)\n\nwith gr.Row():\n  with gr.Column():\n    gr.Image(image_path, elem_id=\"banner-image\", show_label=False)\n  with gr.Column():\n    gr.HTML(description)\n</code></pre>\n\n<p>That pulls together the appâ€™s title, image, description, and custom CSS.</p>\n<h3>Creating The Form Component</h3>\n<p>The app is based on a form element that takes audio from the userâ€™s microphone, then outputs the transcribed text and sentiment analysis formatted based on the userâ€™s selection.</p>\n<p>In Gradio, we define a <code>Group()</code> containing a <a href=\"https://www.gradio.app/docs/block-layouts#box\"><code>Box()</code></a> component. A group is merely a container to hold child components without any spacing. In this case, the <code>Group()</code> is the parent container for a <code>Box()</code> child component, a pre-styled container with a border, rounded corners, and spacing.</p>\n<pre><code>with gr.Group():\n  with gr.Box():\n</code></pre>\n\n<p>With our <code>Box()</code> component in place, we can use it as a container for the audio file form input, the radio buttons for choosing a format for the analysis, and the button to submit the form:</p>\n<pre><code>with gr.Group():\n  with gr.Box():\n    # Audio Input\n    audio = gr.Audio(\n      label=\"Input Audio\",\n      show_label=False,\n      source=\"microphone\",\n      type=\"filepath\"\n    )\n\n    # Sentiment Option\n    sentiment_option = gr.Radio(\n      choices=[\"Sentiment Only\", \"Sentiment + Score\"],\n      label=\"Select an option\",\n      default=\"Sentiment Only\"\n    )\n\n    # Transcribe Button\n    btn = gr.Button(\"Transcribe\")\n</code></pre>\n\n<h3>Output Components</h3>\n<p>Next, we define <a href=\"https://www.gradio.app/docs/textbox\"><code>Textbox()</code> components</a> as output components for the detected language, transcription, and sentiment analysis results.</p>\n<div>\n<pre><code>lang_str = gr.Textbox(label=\"Language\")\ntext = gr.Textbox(label=\"Transcription\")\nsentiment_output = gr.Textbox(label=\"Sentiment Analysis Results\", output=True)\n</code></pre>\n</div>\n\n<h3>Button Action</h3>\n<p>Before we move on to the footer, itâ€™s worth specifying the action executed when the formâ€™s <a href=\"https://www.gradio.app/docs/button#button-click\"><code>Button()</code> component</a> â€” the \"Transcribe\" button â€” is clicked. We want to trigger the fourth function we defined earlier, <code>inference()</code>, using the required inputs and outputs.</p>\n<pre><code>btn.click(\n  inference,\n  inputs=[\n    audio,\n    sentiment_option\n  ],\n  outputs=[\n    lang_str,\n    text,\n    sentiment_output\n  ]\n)\n</code></pre>\n\n<h3>Footer HTML</h3>\n<p>This is the very bottom of the layout, and Iâ€™m giving OpenAI credit with a link to their GitHub repository.</p>\n<div>\n<pre><code>gr.HTML(â€™â€™â€™\n  &lt;div class=\"footer\"&gt;\n    &lt;p&gt;Model by &lt;a href=\"<a href=\"https://github.com/openai/whisper&quot;\">https://github.com/openai/whisper\"</a> style=\"text-decoration: underline;\" target=\"_blank\"&gt;OpenAI&lt;/a&gt;\n    &lt;/p&gt;\n  &lt;/div&gt;\nâ€™â€™â€™)\n</code></pre>\n</div>\n\n<h3>Launch the Block</h3>\n<p>Finally, we launch the Gradio block to render the UI.</p>\n<pre><code>block.launch()\n</code></pre>\n\nHosting &amp; Deployment\n<p>Now that we have successfully built the appâ€™s UI, itâ€™s time to deploy it. Weâ€™ve already used Hugging Face resources, like its Transformers library. In addition to supplying machine learning capabilities, pre-trained models, and datasets, Hugging Face also provides a social hub called <a href=\"https://huggingface.co/spaces\">Spaces</a> for deploying and hosting Python-based demos and experiments.</p>\n<p><img src=\"https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/2-ai-real-time-audio-analysis-hugging-face-spaces.png\" /></p>\n<p>You can use your own host, of course. Iâ€™m using Spaces because itâ€™s so deeply integrated with our stack that it makes deploying this Gradio app a seamless experience.</p>\n<p>In this section, I will walk you through Spaceâ€™s deployment process.</p>\n<h3>Creating A New Space</h3>\n<p>Before we start with deployment, we must <a href=\"https://huggingface.co/new-space\">create a new Space</a>.</p>\n<p>The setup is pretty straightforward but requires a few pieces of information, including:</p>\n<ul>\n<li>A name for the Space (mine is â€œReal-Time-Multilingual-sentiment-analysisâ€),</li>\n<li>A license type for fair use (e.g., a BSD license),</li>\n<li>The SDK (weâ€™re using Gradio),</li>\n<li>The hardware used on the server (the â€œfreeâ€ option is fine), and</li>\n<li>Whether the app is publicly visible to the Spaces community or private.</li>\n</ul>\n<p><img src=\"https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/3-ai-real-time-audio-analysis-hugging-face-spaces-new.png\" /></p>\n<p>Once a Space has been created, it can be cloned, or a remote can be added to its current Git repository.</p>\n<h3>Deploying To A Space</h3>\n<p>We have an app and a Space to host it. Now we need to deploy our files to the Space.</p>\n<p>There are a couple of <a href=\"https://huggingface.co/docs/hub/spaces-overview#creating-a-new-space\">options</a> here. If you already have the <code>app.py</code> and <code>requirements.txt</code> files on your computer, you can use Git from a terminal to commit and push them to your Space by <a href=\"https://huggingface.co/docs/hub/repositories-getting-started#terminal\">following these well-documented steps</a>. Or, If you prefer, you can create <code>app.py</code> and <code>requirements.txt</code> <a href=\"https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui\">directly from the Space in your browser</a>.</p>\n<p>Push your code to the Space, and watch the blue â€œBuildingâ€ status that indicates the app is being processed for production.</p>\n<p><img src=\"https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/ai-real-time-audio-analysis-deploy-building.jpg\" /></p>\nFinal Demo\n<p></p>\n<p></p>\nConclusion\n<p>And thatâ€™s a wrap! Together, we successfully created and deployed an app capable of converting an audio file into plain text, detecting the language, analyzing the transcribed text for emotion, and assigning a score that indicates that emotion.</p>\n<p>We used several tools along the way, including OpenAIâ€™s Whisper for <strong>automatic speech recognition</strong>, four functions for producing a <strong>sentiment analysis</strong>, a pre-trained <strong>machine learning model</strong> called <code>roberta-base-go_emotions</code> that we pulled from the Hugging Space Hub, Gradio as a <strong>UI framework</strong>, and Hugging Face Spaces to <strong>deploy</strong> the work.</p>\n<p>How will you use these real-time, sentiment-scoping capabilities in your work? I see so much potential in this type of technology that Iâ€™m interested to know (and see) what you make and how you use it. Let me know in the comments!</p>\n<h3>Further Reading On SmashingMag</h3>\n<ul>\n<li>â€œ<a href=\"https://www.smashingmagazine.com/2022/12/future-design-human-powered-ai-driven/\">The Future Of Design: Human-Powered Or AI-Driven?</a>,â€ Keima Kai</li>\n<li>â€œ<a href=\"https://www.smashingmagazine.com/2022/10/motion-controls-browser/\">Motion Controls In The Browser</a>,â€ Yaphi Berhanu</li>\n<li>â€œ<a href=\"https://www.smashingmagazine.com/2022/09/javascript-api-guide/\">JavaScript APIs You Donâ€™t Know About</a>,â€ Juan Diego RodrÃ­guez</li>\n<li>â€œ<a href=\"https://www.smashingmagazine.com/2023/05/safest-way-hide-api-keys-react/\">The Safest Way To Hide Your API Keys When Using React</a>,â€ Jessica Joseph</li>\n</ul>","author":"","siteTitle":"Articles on Smashing Magazine â€” For Web Designers And Developers","siteHash":"ab069ca35bf300e9db0da36f49701f66485a5b0d2db0471dfeee07cef6204939","entryHash":"5229bb9374cf3aa061332e85f4c7ff2f7165311da4be65b42dd745fec77b55ec","category":"Tech"}