{"title":"New ‚Äì¬†Amazon Genomics CLI Is Now Open Source and Generally Available","link":"https://aws.amazon.com/blogs/aws/new-amazon-genomics-cli-is-now-open-source-and-generally-available/","date":1632765103000,"content":"<p><strong>Update (October 2021)</strong> ‚Äì Added more information about the workflows developed by the Broad Institute.</p> \n<hr /> \n<p>Less than 70 years separate us from one of the greatest discoveries of all time: the double helix structure of DNA. We now know that DNA is a sort of a twisted ladder composed of four types of compounds, called bases. These four bases are usually identified by an uppercase letter: adenine (A), guanine (G), cytosine (C), and thymine (T). One of the reasons for the double helix structure is that when these compounds are at the two sides of the ladder, A always bonds with T, and C always bonds with G.</p> \n<p>If we unroll the ladder on a table, we‚Äôd see two sequences of ‚Äúletters‚Äù, and each of the two sides would carry the same genetic information. For example, here are two series (<code>AGCT</code> and <code>TCGA</code>) bound together:</p> \n<div> \n <pre><code>A ‚Äì T\nG ‚Äì C\nC ‚Äì G\nT ‚Äì A</code></pre> \n</div> \n<p>These series of letters can be very long. For example, the human genome is composed of over 3 billion letters of code and acts as the biological blueprint of every cell in a person. The information in a person‚Äôs genome can be used to create highly personalized treatments to improve the health of individuals and even the entire population. Similarly, genomic data can be use to track infectious diseases, improve diagnosis, and even track epidemics, food pathogens and toxins. This is the emerging field of environmental genomics.</p> \n<p>Accessing genomic data requires genome sequencing, which with recent advances in technology, can be done for large groups of individuals, quickly and more cost-effectively than ever before. In the next five years, genomics datasets are estimated to grow and contain more than a billion sequenced genomes.</p> \n<p><span><strong>How Genomics Data Analysis Works<br /> </strong></span>Genomics data analysis uses a variety of tools that need to be orchestrated as a specific sequence of steps, or a workflow. To facilitate developing, sharing, and running workflows, the genomics and bioinformatics communities have developed specialized workflow definition languages like <a href=\"https://openwdl.org/\">WDL</a>, <a href=\"https://www.nextflow.io/\">Nextflow</a>, <a href=\"https://www.commonwl.org/\">CWL</a>, and <a href=\"https://snakemake.readthedocs.io/en/stable/\">Snakemake</a>.</p> \n<p>However, this process generates petabytes of raw genomic data and experts in genomics and life science struggle to scale compute and storage resources to handle data at such massive scale.</p> \n<p>To process data and provide answers quickly, cloud resources like compute, storage, and networking need to be configured to work together with analysis tools. As a result, scientists and researchers often have to spend valuable time deploying infrastructure and modifying open-source genomics analysis tools instead of making contributions to genomics innovations.</p> \n<p><span><strong>Introducing Amazon Genomics CLI<br /> </strong></span>A couple of months ago, we <a href=\"https://aws.amazon.com/blogs/industries/announcing-amazon-genomics-cli-preview/\">shared the preview</a> of <a href=\"https://aws.amazon.com/genomics-cli/\">Amazon Genomics CLI</a>, a tool that makes it easier to process <a href=\"https://aws.amazon.com/health/genomics/\">genomics</a> data at petabyte scale on AWS. I am excited to share that the <a href=\"https://aws.github.io/amazon-genomics-cli/\">Amazon Genomics CLI is now an open source project</a> and is <strong>generally available</strong> today. You can use it with publicly available workflows as a starting point and develop your analysis on top of these.</p> \n<p>Amazon Genomics CLI simplifies and automates the deployment of cloud infrastructure, providing you with an easy-to-use command line interface to quickly setup and run genomics workflows on AWS. By removing the heavy lifting from setting up and running genomics workflows in the cloud, software developers and researchers can automatically provision, configure and scale cloud resources to enable faster and more cost-effective population-level genetics studies, drug discovery cycles, and more.</p> \n<p>Amazon Genomics CLI lets you run your workflows on an optimized cloud infrastructure. More specifically, the CLI:</p> \n<ul> \n <li>Includes improvements to genomics workflow engines to make them integrate better with AWS, removing the burden to manually modify open-source tools and tune them to run efficiently at scale. These tools work seamlessly across <a href=\"https://aws.amazon.com/ecs/\">Amazon Elastic Container Service (Amazon ECS)</a>, <a href=\"https://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a>, <a href=\"https://aws.amazon.com/efs\">Amazon Elastic File System (Amazon EFS)</a>, and <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a>, helping you to scale compute and storage and at the same time optimize your costs using features like <a href=\"https://aws.amazon.com/ec2/spot/\">EC2 Spot Instances</a>.</li> \n <li>Eliminates the most time-consuming tasks like provisioning storage and compute capacities, deploying the genomics workflow engines, and tuning the clusters used to execute workflows.</li> \n <li>Automatically increases or decreases cloud resources based on your workloads, which eliminates the risk of buying too much or too little capacity.</li> \n <li>Tags resources so that you can use tools like <a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-and-usage-reporting/\">AWS Cost &amp; Usage Report</a> to understand the costs related to your genomics data analysis across multiple AWS services.</li> \n</ul> \n<p>The use of Amazon Genomics CLI is based on these three main concepts:</p> \n<p><strong>Workflow</strong> ‚Äì These are bioinformatics workflows written in languages like WDL or Nextflow. They can be either single script files or packages of multiple files. These workflow script files are <strong>workflow definitions</strong> and combined with additional metadata, like the workflow language the definition is written in, form a <strong>workflow specification</strong> that is used by the CLI to execute workflows on appropriate compute resources.</p> \n<p><strong>Context</strong> ‚Äì A context encapsulates and automates time-consuming tasks to configure and deploy workflow engines, create data access policies, and tune compute clusters (managed using <a href=\"http://aws.amazon.com/batch\">AWS Batch</a>) for operation at scale.</p> \n<p><strong>Project</strong> ‚Äì A project links together workflows, datasets, and the contexts used to process them. From a user perspective, it handles resources related to the same problem or used by the same team.</p> \n<p>Let‚Äôs see how this works in practice.</p> \n<p><span><strong>Using Amazon Genomics CLI<br /> </strong></span>I follow the instructions to install Amazon Genomics CLI on my laptop. Now, I can use the <code>agc</code> command to manage genomic workloads. I see the available options with:</p> \n<div> \n <pre><code>$ agc --help</code></pre> \n</div> \n<p>The first time I use it, I activate my AWS account:</p> \n<div> \n <pre><code>$ agc account activate</code></pre> \n</div> \n<p>This creates the core infrastructure that Amazon Genomics CLI needs to operate, which includes an S3 bucket, a <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\">virtual private cloud (VPC)</a>, and a DynamoDB table. The S3 bucket is used for durable metadata, and the VPC is used to isolate compute resources.</p> \n<p>Optionally, I can bring my own VPC. I can also use one of my <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\">named profiles</a> for the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (CLI)</a>. In this way, I can customize the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Region</a> and the AWS account used by the Amazon Genomics CLI.</p> \n<p>I configure my email address in the local settings. This wil be used to tag resources created by the CLI:</p> \n<div> \n <pre><code>$ agc configure email me@example.net</code></pre> \n</div> \n<p>There are a few demo projects in the examples folder included by the Amazon Genomics CLI installation. These projects use different engines, such as <a href=\"https://cromwell.readthedocs.io/en/develop/\">Cromwell</a> or <a href=\"https://www.nextflow.io/\">Nextflow</a>. In the <code>demo-wdl-project</code> folder, the <code>agc-project.yaml</code> file describes the workflows, the data, and the contexts for the <code>Demo</code> project:</p> \n<pre><code>---\nname: Demo\nschemaVersion: 1\nworkflows:\n  hello:\n    type:\n      language: wdl\n      version: 1.0\n    sourceURL: workflows/hello\n  read:\n    type:\n      language: wdl\n      version: 1.0\n    sourceURL: workflows/read\n  haplotype:\n    type:\n      language: wdl\n      version: 1.0\n    sourceURL: workflows/haplotype\n  words-with-vowels:\n    type:\n      language: wdl\n      version: 1.0\n    sourceURL: workflows/words\ndata:\n  - location: s3://gatk-test-data\n    readOnly: true\n  - location: s3://broad-references\n    readOnly: true\ncontexts:\n  myContext:\n    engines:\n      - type: wdl\n        engine: cromwell\n\n  spotCtx:\n    requestSpotInstances: true\n    engines:\n      - type: wdl\n        engine: cromwell</code></pre> \n<p>For this project, there are four workflows (<code>hello</code>, <code>read</code>, <code>words-with-vowels</code>, and <code>haplotype</code>). The project has read-only access to two S3 buckets and can run workflows using two contexts. Both contexts use the Cromwell engine. One context (<code>spotCtx</code>) uses <a href=\"https://aws.amazon.com/ec2/spot/\">Amazon EC2 Spot Instances</a> to optimize costs.</p> \n<p>In the <code>demo-wdl-project</code> folder, I use the Amazon Genomics CLI to deploy the <code>spotCtx</code> context:</p> \n<div> \n <pre><code>$ agc context deploy -c spotCtx</code></pre> \n</div> \n<p>After a few minutes, the context is ready, and I can execute the workflows. Once started, a context incurs about $0.40 per hour of baseline costs. These costs don‚Äôt include the resources created to execute workflows. Those resources depend on your specific use case. Contexts have the option to use spot instances by adding the <code>requestSpotInstances</code>¬†flag to their configuration.</p> \n<p>I use the CLI to see the status of the contexts of the project:</p> \n<div> \n <pre><code>$ agc context status\n\nINSTANCE spotCtx STARTED true</code></pre> \n</div> \n<p>Now, let‚Äôs look at the workflows included in this project:</p> \n<div> \n <pre><code>$ agc workflow list\n\n2021-09-24T11:15:29+01:00 ùíä¬† Listing workflows.\nWORKFLOWNAME haplotype\nWORKFLOWNAME hello\nWORKFLOWNAME read\nWORKFLOWNAME words-with-vowels</code></pre> \n</div> \n<p>The simplest workflow is <code>hello</code>. The content of the <code>hello.wdl</code> file is quite understandable if you know any programming language:</p> \n<div> \n <pre><code>version 1.0\nworkflow hello_agc {\n    call hello {}\n}\ntask hello {\n    command { echo \"Hello Amazon Genomics CLI!\" }\n    runtime {\n        docker: \"ubuntu:latest\"\n    }\n    output { String out = read_string( stdout() ) }\n}</code></pre> \n</div> \n<p>The <code>hello</code> workflow defines a single task (<code>hello</code>) that prints the output of a command. The task is executed on a specific container image (<code>ubuntu:latest</code>). The output is taken from standard output (<code>stdout</code>), the default file descriptor where a process can write output.</p> \n<p>Running workflows is an asynchronous process. After submitting a workflow from the CLI, it is handled entirely in the cloud. I can run multiple workflows at a time. The underlying compute resources will automatically scale and I will be charged only for what I use.</p> \n<p>Using the CLI, I start the <code>hello</code> workflow:</p> \n<div> \n <div> \n  <pre><code>$ agc workflow run hello -c spotCtx\n\n2021-09-24T13:03:47+01:00 ùíä¬† Running workflow. Workflow name: 'hello', Arguments: '', Context: 'spotCtx'\nfcf72b78-f725-493e-b633-7dbe67878e91</code></pre> \n </div> \n</div> \n<p>The workflow was successfully submitted, and the last line is the <strong>workflow execution ID</strong>. I can use this ID to reference a specific workflow execution. Now, I check the status of the workflow:</p> \n<div> \n <div> \n  <pre><code>$ agc workflow status\n\n2021-09-24T13:04:21+01:00 ùíä  Showing workflow run(s). Max Runs: 20\nWORKFLOWINSTANCE\tspotCtx\tfcf72b78-f725-493e-b633-7dbe67878e91\ttrue\tRUNNING\t2021-09-24T12:03:53Z\thello</code></pre> \n </div> \n</div> \n<p>The <code>hello</code> workflow is still running. After a few minutes, I check again:</p> \n<div> \n <pre><code>$ agc workflow status\n\n2021-09-24T13:12:23+01:00 ùíä  Showing workflow run(s). Max Runs: 20\nWORKFLOWINSTANCE\tspotCtx\tfcf72b78-f725-493e-b633-7dbe67878e91\ttrue\tCOMPLETE\t2021-09-24T12:03:53Z\thello</code></pre> \n</div> \n<p>The workflow has terminated and is now complete. I look at the workflow logs:</p> \n<div> \n <pre><code>$ agc logs workflow hello\n\n2021-09-24T13:13:08+01:00 ùíä  Showing the logs for 'hello'\n2021-09-24T13:13:12+01:00 ùíä  Showing logs for the latest run of the workflow. Run id: 'fcf72b78-f725-493e-b633-7dbe67878e91'\nFri, 24 Sep 2021 13:07:22 +0100\tdownload: s3://agc-123412341234-eu-west-1/scripts/1a82f9a96e387d78ae3786c967f97cc0 to tmp/tmp.498XAhEOy/batch-file-temp\nFri, 24 Sep 2021 13:07:22 +0100\t*** LOCALIZING INPUTS ***\nFri, 24 Sep 2021 13:07:23 +0100\tdownload: s3://agc-123412341234-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/script to agc-024700040865-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/script\nFri, 24 Sep 2021 13:07:23 +0100\t*** COMPLETED LOCALIZATION ***\nFri, 24 Sep 2021 13:07:23 +0100\tHello Amazon Genomics CLI!\nFri, 24 Sep 2021 13:07:23 +0100\t*** DELOCALIZING OUTPUTS ***\nFri, 24 Sep 2021 13:07:24 +0100\tupload: ./hello-rc.txt to s3://agc-123412341234-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/hello-rc.txt\nFri, 24 Sep 2021 13:07:25 +0100\tupload: ./hello-stderr.log to s3://agc-123412341234-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/hello-stderr.log\nFri, 24 Sep 2021 13:07:25 +0100\tupload: ./hello-stdout.log to s3://agc-123412341234-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/hello-stdout.log\nFri, 24 Sep 2021 13:07:25 +0100\t*** COMPLETED DELOCALIZATION ***\nFri, 24 Sep 2021 13:07:25 +0100\t*** EXITING WITH RETURN CODE ***\nFri, 24 Sep 2021 13:07:25 +0100\t0\n</code></pre> \n</div> \n<p>In the logs, I find as expected the <code>Hello Amazon Genomics CLI!</code> message printed by workflow.</p> \n<p>I can also look at the content of <code>hello-stdout.log</code> on S3 using the information in the log above:</p> \n<div> \n <pre><code>aws s3 cp s3://agc-123412341234-eu-west-1/project/Demo/userid/danilop20tbvT/context/spotCtx/cromwell-execution/hello_agc/fcf72b78-f725-493e-b633-7dbe67878e91/call-hello/hello-stdout.log -\n\nHello Amazon Genomics CLI!</code></pre> \n</div> \n<p>It worked! Now, let‚Äôs look for at more complex workflows. Before I change project, I destroy the context for the <code>Demo</code> project:</p> \n<div> \n <pre><code>$ agc context destroy -c spotCtx</code></pre> \n</div> \n<p>In the <code>gatk-best-practices-project</code> folder, I list the available workflows for the project:</p> \n<div> \n <pre><code>$ agc workflow list\n\n2021-09-24T11:41:14+01:00 ùíä  Listing workflows.\nWORKFLOWNAME\tbam-to-unmapped-bams\nWORKFLOWNAME\tcram-to-bam\nWORKFLOWNAME\tgatk4-basic-joint-genotyping\nWORKFLOWNAME\tgatk4-data-processing\nWORKFLOWNAME\tgatk4-germline-snps-indels\nWORKFLOWNAME\tgatk4-rnaseq-germline-snps-indels\nWORKFLOWNAME\tinterleaved-fastq-to-paired-fastq\nWORKFLOWNAME\tpaired-fastq-to-unmapped-bam\nWORKFLOWNAME\tseq-format-validation</code></pre> \n</div> \n<p>This project includes workflows based on <a href=\"//gatk.broadinstitute.org/hc/en-us\">GATK Best Practices</a>, developed by the <a href=\"https://www.broadinstitute.org/\">Broad Institute</a>. More information on how these workflows work is available in the <a href=\"https://github.com/gatk-workflows\">GATK Workflows GitHub repository</a>.</p> \n<p>In the <code>agc-project.yaml</code> file, the <code>gatk4-data-processing</code> workflow points to a local directory with the same name. This is the content of that directory:</p> \n<div> \n <pre><code>$ ls gatk4-data-processing\n\nMANIFEST.json\nprocessing-for-variant-discovery-gatk4.hg38.wgs.inputs.json\nprocessing-for-variant-discovery-gatk4.wdl</code></pre> \n</div> \n<p>This workflow processes high-throughput sequencing data with <a href=\"https://gatk.broadinstitute.org/\">GATK4</a>, a genomic analysis toolkit focused on variant discovery.</p> \n<p>The directory contains a <code>MANIFEST.json</code> file. The manifest file describes which file contains the main workflow to execute (there can be more than one WDL file in the directory) and where to find input parameters and options. Here‚Äôs the content of the manifest file:</p> \n<div> \n <pre><code>{\n  \"mainWorkflowURL\": \"processing-for-variant-discovery-gatk4.wdl\",\n  \"inputFileURLs\": [\n    \"processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json\"\n  ],\n  \"optionFileURL\": \"options.json\"\n}</code></pre> \n</div> \n<p>In the <code>gatk-best-practices-project</code> folder, I create a context to run the workflows:</p> \n<div> \n <pre><code>$ agc context deploy -c spotCtx</code></pre> \n</div> \n<p>Then, I start the <code>gatk4-data-processing</code> workflow:</p> \n<div> \n <pre><code>$ agc workflow run gatk4-data-processing -c spotCtx\n\n2021-09-24T12:08:22+01:00 ùíä  Running workflow. Workflow name: 'gatk4-data-processing', Arguments: '', Context: 'spotCtx'\n630e2d53-0c28-4f35-873e-65363529c3de</code></pre> \n</div> \n<p>After a couple of hours, the workflow has terminated:</p> \n<div> \n <pre><code>$ agc workflow status\n\n2021-09-24T14:06:40+01:00 ùíä  Showing workflow run(s). Max Runs: 20\nWORKFLOWINSTANCE\tspotCtx\t630e2d53-0c28-4f35-873e-65363529c3de\ttrue\tCOMPLETE\t2021-09-24T11:08:28Z\tgatk4-data-processing</code></pre> \n</div> \n<p>I look at the logs:</p> \n<div> \n <pre><code>$ agc logs workflow gatk4-data-processing\n\n...\nFri, 24 Sep 2021 14:02:32 +0100\t*** DELOCALIZING OUTPUTS ***\nFri, 24 Sep 2021 14:03:45 +0100\tupload: ./NA12878.hg38.bam to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/NA12878.hg38.bam\nFri, 24 Sep 2021 14:03:46 +0100\tupload: ./NA12878.hg38.bam.md5 to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/NA12878.hg38.bam.md5\nFri, 24 Sep 2021 14:03:47 +0100\tupload: ./NA12878.hg38.bai to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/NA12878.hg38.bai\nFri, 24 Sep 2021 14:03:48 +0100\tupload: ./GatherBamFiles-rc.txt to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/GatherBamFiles-rc.txt\nFri, 24 Sep 2021 14:03:49 +0100\tupload: ./GatherBamFiles-stderr.log to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/GatherBamFiles-stderr.log\nFri, 24 Sep 2021 14:03:50 +0100\tupload: ./GatherBamFiles-stdout.log to s3://agc-123412341234-eu-west-1/project/GATK/userid/danilop20tbvT/context/spotCtx/cromwell-execution/PreProcessingForVariantDiscovery_GATK4/630e2d53-0c28-4f35-873e-65363529c3de/call-GatherBamFiles/GatherBamFiles-stdout.log\nFri, 24 Sep 2021 14:03:50 +0100\t*** COMPLETED DELOCALIZATION ***\nFri, 24 Sep 2021 14:03:50 +0100\t*** EXITING WITH RETURN CODE ***\nFri, 24 Sep 2021 14:03:50 +0100\t0</code></pre> \n</div> \n<p>Results have been written to the S3 bucket created during the account activation. The name of the bucket is in the logs but I can also find it stored as a parameter by <a href=\"https://aws.amazon.com/systems-manager/\">AWS Systems Manager</a>. I can save it in an environment variable with the following command:</p> \n<div> \n <pre><code>$ export AGC_BUCKET=$(aws ssm get-parameter \\\n  --name /agc/_common/bucket \\\n  --query 'Parameter.Value' \\\n  --output text)</code></pre> \n</div> \n<p>Using the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (CLI)</a>, I can now explore the results on the S3 bucket and get the outputs of the workflow.</p> \n<p>Before looking at the results, I remove the resources that I don‚Äôt need by stopping the context. This will destroy all compute resources, but retain data in S3.</p> \n<div> \n <pre><code>$ agc context destroy -c spotCtx</code></pre> \n</div> \n<p>Additional examples on configuring different contexts and running additional workflows are provided in the <a href=\"https://aws.github.io/amazon-genomics-cli/\">documentation on GitHub</a>.</p> \n<p><span><strong>Availability and Pricing<br /> </strong></span><a href=\"https://aws.amazon.com/genomics-cli/\">Amazon Genomics CLI</a> is an <a href=\"https://aws.github.io/amazon-genomics-cli/\">open source tool</a>, and you can use it today in all <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Regions</a> with the exception of AWS GovCloud (US) and Regions located in China. There is no cost for using the AWS Genomics CLI. You pay for the AWS resources created by the CLI.</p> \n<p>With the Amazon Genomics CLI, you can focus on science instead of architecting infrastructure. This gets you up and running faster, enabling research, development, and testing workloads. For production workloads that scale to several thousand parallel workflows, we can provide recommended ways to leverage additional Amazon services, like <a href=\"https://aws.amazon.com/step-functions\">AWS Step Functions</a>, just <a href=\"https://aws.amazon.com/contact-us/sales-support/\">reach out to our account teams</a>¬†for more information.</p> \n<p>‚Äî <a href=\"https://twitter.com/danilop\">Danilo</a></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"78271bb8faa1b9693bb62c34f6620609b6494ccb0dee9b700f018b27c2ceb83d","category":"Tech"}