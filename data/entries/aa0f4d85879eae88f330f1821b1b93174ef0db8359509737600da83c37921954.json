{"title":"Evaluation Metrics for Machine Unlearning","link":"https://www.preprints.org/manuscript/202409.1925/v1","date":1727189905000,"content":"The evaluation of machine unlearning has become increasingly significant as machine learning systems face growing demands for privacy, security, and regulatory compliance. This paper focuses on categorizing and analyzing evaluation metrics for machine unlearning, essential for assessing the success of unlearning processes. We divide the metrics into three key dimensions: unlearning effectiveness, unlearning efficiency, and model utility. Unlearning effectiveness examines the degree to which data is removed from the model, utilizing methods such as data removal completeness, privacy leakage detection, and perturbation analysis to ensure thorough data erasure. Unlearning efficiency considers metrics like time to unlearn, computational cost, and scalability, which are crucial for maintaining system performance in real-time environments. Model utility metrics, including accuracy retention, robustness, and fairness, ensure that unlearning does not compromise the modelâ€™s predictive capabilities. Through this categorization, we present a comprehensive framework for evaluating machine unlearning, providing a foundation for developing unlearning techniques that balance privacy, performance, and regulatory needs across diverse industries, particularly finance.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"aa0f4d85879eae88f330f1821b1b93174ef0db8359509737600da83c37921954","category":"Interdisciplinary"}