{"title":"Statistical Signatures of Abstraction in Deep Neural Networks","link":"https://www.preprints.org/manuscript/202407.0253/v1","date":1719977530000,"content":"We study how abstract representations emerge in a Deep Belief Network (DBN) trained on benchmark datasets. Our analysis targets the principles of learning in the early stages of information processing, starting from the “primordial soup\" of the under-sampling regime. As the data is processed by deeper and deeper layers, features are detected and removed, transferring more and more “context-invariant” information to deeper layers. We show that the representation approaches an universal model – the Hierarchical Feature Model (HFM) – determined by the principle of maximal relevance. Relevance quantifies the uncertainty on the model of the data, thus suggesting that “meaning\" – i.e. syntactic information – is that part of the data which is not yet captured by a model. Our analysis shows that shallow layers are well described by pairwise Ising models, which provide a representation of the data in terms of generic, low order features. We also show that plasticity increases with depth, in a similar way as it does in the brain. These findings suggest that DBNs are capable of extracting a hierarchy of features from the data which is consistent with the principle of maximal relevance.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"eda59bc62123f21777e1a7adca4bd8b4f39888697bb90f5bf978d1cfa4a965ca","category":"Interdisciplinary"}