{"title":"Multimodal pretraining for unsupervised protein representation learning","link":"https://academic.oup.com/biomethods/article/doi/10.1093/biomethods/bpae043/7695866?rss=1","date":1718668800000,"content":"<span><div>Abstract</div>Proteins are complex biomolecules essential for numerous biological processes, making them crucial targets for advancements in molecular biology, medical research, and drug design. Understanding their intricate, hierarchical structures, and functions is vital for progress in these fields. To capture this complexity, we introduce Multimodal Protein Representation Learning (MPRL), a novel framework for symmetry-preserving multimodal pretraining that learns unified, unsupervised protein representations by integrating primary and tertiary structures. MPRL employs Evolutionary Scale Modeling (ESM-2) for sequence analysis, Variational Graph Auto-Encoders (VGAE) for residue-level graphs, and PointNet Autoencoder (PAE) for 3D point clouds of atoms, each designed to capture the spatial and evolutionary intricacies of proteins while preserving critical symmetries. By leveraging Auto-Fusion to synthesize joint representations from these pretrained models, MPRL ensures robust and comprehensive protein representations. Our extensive evaluation demonstrates that MPRL significantly enhances performance in various tasks such as proteinâ€“ligand binding affinity prediction, protein fold classification, enzyme activity identification, and mutation stability prediction. This framework advances the understanding of protein dynamics and facilitates future research in the field. Our source code is publicly available at <a href=\"https://github.com/HySonLab/Protein_Pretrain\">https://github.com/HySonLab/Protein_Pretrain</a>.</span>","author":"","siteTitle":"Biology Methods and Protocols Current Issue","siteHash":"69c994dba77628d461517adf354c299799d03c68fbf0494d0a5619d40a95f351","entryHash":"193993aeff9ff0e712fceb3dd776ca5a62b22bfcd621ebad0f4200940dbc5770","category":"Environment"}