{"title":"Enhancing Multimodal Emotion Recognition through Attention Mechanisms in BERT and CNN Architectures","link":"https://www.preprints.org/manuscript/202404.1574/v1","date":1713940153000,"content":"Emotion detection holds significant importance in facilitating human-computer interaction, enhancing the depth of engagement. By integrating this capability, we pave the way for forthcoming AI technologies to possess a blend of cognitive and emotional understanding, bridging the divide between machine functionality and human emotional complexity. This progress has the potential to reshape how machines perceive and respond to human emotions, ushering in an era of empathetic and intuitive artificial systems. This paper introduces a novel approach to multimodal emotion recognition, seamlessly integrating speech and text modalities to accurately infer emotional states. Employing CNNs, we meticulously analyze speech using Mel spectrograms, while a BERT-based model processes the textual component, leveraging its bidirectional layers for profound semantic comprehension. The outputs from both modalities are combined using an attention-based fusion mechanism that optimally weighs their contributions. The proposed method undergoes meticulous testing on two distinct datasets: Carnegie Mellon Universityâ€™s Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset and the Multimodal Emotion Lines Dataset (MELD). The results demonstrate superior efficacy compared to existing frameworks, achieving an accuracy of 88.4% and an F1-score of 87.9% on the CMU-MOSEI dataset, and a notable weighted accuracy (WA) of 67.81% and a weighted F1 (WF1) score of 66.32% on the MELD dataset. This comprehensive system offers precise emotion detection and introduces several significant advancements in the field.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"4c8fc97dd1e715285458b5a3303d9f94d5be1ed2ba2d0ace7bd0a6b7442f1b4b","category":"Interdisciplinary"}