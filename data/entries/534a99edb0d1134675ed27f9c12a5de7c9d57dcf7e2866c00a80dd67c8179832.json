{"title":"Scaffolding learning: From specific to generic with large language models","link":"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0310409","date":1726840800000,"content":"<p>by David S. Yin, Xiaoxin Yin</p>\r\n\r\nLarge language models such as ChatGPT have been shown to excel in solving complex math problems. However, they cannot solve basic arithmetic problems such as 758*639 = 484,362. This makes us ponder if LLMs have been trained to solve math and science problems in the right way. When a student learns math at school, she or he starts with arithmetic, then moves to word problems, polynomials, and calculus. Each skill she or he acquires will be used in the next stage to solve more advanced problems. In this paper we propose <i>Scaffolding Learning</i> for LLMs, which imitates how a student learns a subject in a step-by-step manner. For example, we first train an LLM to perform highly specific operations such as multiplication and division, and then apply such “skills” in a more generic task such as solving word problems. This is related to <i>Curriculum Training</i>, which trains a model on tasks following a specific order, such as training on easy tasks first and then gradually increases the difficulty. Our proposed approach goes from specific tasks to generic ones, which can be considered as a special case of Curriculum Training. Our empirical studies show that when an LLM has “mastered” a specific skill, only a small amount of training is required to teach it to apply the skill to a more generic application.","author":"David S. Yin","siteTitle":"PLOS ONE","siteHash":"e9ab556ceb1e4ea76e897a5fa4f394f0bb75c2c2f3d5b0f4766ff77b4a262ac1","entryHash":"534a99edb0d1134675ed27f9c12a5de7c9d57dcf7e2866c00a80dd67c8179832","category":"Interdisciplinary"}