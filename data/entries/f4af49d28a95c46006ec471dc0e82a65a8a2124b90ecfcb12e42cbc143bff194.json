{"title":"Attention-Linear Trajectory Prediction","link":"https://www.preprints.org/manuscript/202408.1892/v1","date":1724747148000,"content":"Recently, a large number of Transformer-based solutions have emerged for the trajectory prediction task, but there are shortcomings in the effectiveness of Transformers in trajectory prediction. Specifically, while position encoding preserves some of the ordering information, the self-attention mechanism at the core of the Transformer has its alignment invariance that leads to the loss of temporal information, which is crucial for trajectory prediction. For this reason, we design a simple and efficient strategy for temporal information extraction and prediction of trajectory sequences using the self-attention mechanism and linear layers. Experimental results show that the strategy we designed can effectively combine the advantages of the linear layer and self-attention mechanisms, compensating for the shortcomings of the Transformer. Additionally, we conducted an empirical study to explore the effectiveness of the linear layer and sparse self-attention mechanisms in trajectory prediction.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"f4af49d28a95c46006ec471dc0e82a65a8a2124b90ecfcb12e42cbc143bff194","category":"Interdisciplinary"}