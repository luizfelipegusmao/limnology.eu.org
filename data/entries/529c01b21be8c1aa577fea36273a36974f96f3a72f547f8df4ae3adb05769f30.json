{"title":"Integrated Optimization of Large Language Models: Synergizing Data Utilization and Compression Techniques","link":"https://www.preprints.org/manuscript/202409.0662/v2","date":1725970615000,"content":"In this paper, we propose \"Synergized Efficiency Optimization for Large Language Models\" (SEO-LLM), a groundbreaking approach that integrates advanced data utilization and model compression techniques to significantly enhance the performance, efficiency, and scalability of large language models (LLMs). Our method synergistically combines Adaptive Data Augmentation (ADA), Transfer- Active Learning (TAL), Adaptive Iterative Pruning (AIP), and Synergistic Quantization and Distillation (SQD). These components work together to reduce the training data requirement by 30%, compress model size by 67.6%, and improve inference speed by up to 50%, while preserving or even enhancing model accuracy across various NLP tasks. ADA dynamically adjusts augmentation strategies to optimize model generalization, while TAL leverages pre-trained models to focus learning on the most informative data samples. AIP intelligently prunes less significant weights, and SQD harmonizes quantization with knowledge distillation to achieve high compression rates without significant performance loss. The synergy between these techniques makes SEO-LLM a robust solution for deploying LLMs in resource-constrained environments, maintaining state-of-the-art performance with a fraction of the computational and data resources.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"529c01b21be8c1aa577fea36273a36974f96f3a72f547f8df4ae3adb05769f30","category":"Interdisciplinary"}