{"title":"GenerRNA: A generative pre-trained language model for <i>de novo</i> RNA design","link":"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0310814","date":1727791200000,"content":"<p>by Yichong Zhao, Kenta Oono, Hiroki Takizawa, Masaaki Kotera</p>\r\n\r\nThe design of RNA plays a crucial role in developing RNA vaccines, nucleic acid therapeutics, and innovative biotechnological tools. However, existing techniques frequently lack versatility across various tasks and are dependent on pre-defined secondary structure or other prior knowledge. To address these limitations, we introduce GenerRNA, a Transformer-based model inspired by the success of large language models (LLMs) in protein and molecule generation. GenerRNA is pre-trained on large-scale RNA sequences and capable of generating novel RNA sequences with stable secondary structures, while ensuring distinctiveness from existing sequences, thereby expanding our exploration of the RNA space. Moreover, GenerRNA can be fine-tuned on smaller, specialized datasets for specific subtasks, enabling the generation of RNAs with desired functionalities or properties without requiring any prior knowledge input. As a demonstration, we fine-tuned GenerRNA and successfully generated novel RNA sequences exhibiting high affinity for target proteins. Our work is the first application of a generative language model to RNA generation, presenting an innovative approach to RNA design.","author":"Yichong Zhao","siteTitle":"PLOS ONE","siteHash":"e9ab556ceb1e4ea76e897a5fa4f394f0bb75c2c2f3d5b0f4766ff77b4a262ac1","entryHash":"519ba7e8fce746ba906ffd2eb3abf534f9991db8e2c49d3fe45da4c46ff26769","category":"Interdisciplinary"}