{"title":"Accelerating and Compressing Transformer-based PLMs for Enhanced Comprehension of Computer Terminology","link":"https://www.preprints.org/manuscript/202409.2415/v1","date":1727699826000,"content":"Pre-trained language models (PLMs) have significantly advanced natural language processing (NLP), establishing the \"pre-training + fine-tuning\" paradigm as a cornerstone approach in the field. However, the vast size and computational demands of Transformer-based PLMs present challenges, particularly regarding storage efficiency and processing speed. This paper addresses these limitations by proposing a novel lightweight PLM optimized for accurately understanding domain-specific computer terminology. Our method involves a pipeline parallelism algorithm designed to accelerate training. It is paired with an innovative mixed compression strategy that combines pruning and knowledge distillation to effectively reduce the model size while preserving its performance. The model is further fine-tuned using a dataset that mixes source and target languages to enhance its versatility. Comprehensive experimental evaluations demonstrate that the proposed approach successfully achieves a balance between model efficiency and performance, offering a scalable solution for NLP tasks involving specialized terminology.","author":"","siteTitle":"Preprints.org - The Multidisciplinary Preprint Platform","siteHash":"abac34b0506002eba4392ac15186820b9b5d7a0f2e5fce3a3511408258fb1a7e","entryHash":"f462efe8431e7765c60cc5dadfeeec2819ca62dc9823bbe58dd54fd82448a6d9","category":"Interdisciplinary"}